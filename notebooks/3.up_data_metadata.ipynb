{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load common functions, libs and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version - 15/02/19 14:18\n"
     ]
    }
   ],
   "source": [
    "print(\"version - 15/02/19 14:18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/intake/source/discovery.py:39: UserWarning: Plugin name collision for \"netcdf\" from\n",
      "    /opt/conda/lib/python3.6/site-packages/intake_iris/netcdf.py\n",
      "and\n",
      "    /opt/conda/lib/python3.6/site-packages/intake_xarray/netcdf.py\n",
      "Keeping plugin from first location.\n",
      "  % (plugin_name, orig_path, new_path))\n"
     ]
    }
   ],
   "source": [
    "# %load common.py\n",
    "import iris\n",
    "import xarray\n",
    "import os\n",
    "from numcodecs import Blosc\n",
    "import s3fs\n",
    "import zarr\n",
    "import intake\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import cf_units\n",
    "import json\n",
    "import  dask_kubernetes\n",
    "import distributed\n",
    "import boto3\n",
    "from iris.experimental.equalise_cubes import equalise_attributes\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.normpath(os.getcwd()))\n",
    "from offsetmap import OffSetS3Map\n",
    "\n",
    "sqs = boto3.client('sqs')\n",
    "\n",
    "AWS_EARTH_TIME_FORMAT = '%Y-%m-%dT%H:%M:%SZ'\n",
    "# SQS_QUEUE_URL = 'https://sqs.eu-west-2.amazonaws.com/536099501702/aws-earth-test'\n",
    "SQS_QUEUE_URL = 'https://sqs.eu-west-2.amazonaws.com/536099501702/rolling_zarr_test_queue'\n",
    "\n",
    "BUCKET = \"metoffice-aws-earth-zarr\"\n",
    "\n",
    "def del_msg(msg):\n",
    "    sqs.delete_message(\n",
    "        QueueUrl=SQS_QUEUE_URL,\n",
    "        ReceiptHandle=msg['receipt_handle']\n",
    "    )\n",
    "    \n",
    "def get_messages(max_num=10):\n",
    "    res = sqs.receive_message(QueueUrl=SQS_QUEUE_URL, MaxNumberOfMessages=max_num, VisibilityTimeout=60*10)\n",
    "    messages  = []\n",
    "    for message in res['Messages']:\n",
    "        msg = json.loads(message['Body'])\n",
    "        msg['receipt_handle'] = message['ReceiptHandle']\n",
    "        messages.append(msg)\n",
    "    return messages\n",
    "\n",
    "def get_zar_path(meta):\n",
    "    base = f\"{BUCKET}/{meta['model']}-{meta['name']}\"\n",
    "    if meta.get('cell_methods', False):\n",
    "        base += f\"-{meta['cell_methods']}\"\n",
    "    if  meta.get('height', False) and (len(meta['height'].strip().split(' ')) > 1):\n",
    "        base += '-at_heights'\n",
    "    if meta.get('pressure', False) and (len(meta['pressure'].strip().split(' ')) > 1):\n",
    "        base += '-at_pressures'\n",
    "    return base + '.zarr'\n",
    "    \n",
    "\n",
    "def zarr_store(meta):\n",
    "    return OffSetS3Map(root=get_zar_path(meta), temp_chunk_path=meta['name'], check=False)\n",
    "    \n",
    "def msg_to_path(msg):\n",
    "    return f'/s3/{msg[\"bucket\"]}/{msg[\"key\"]}'\n",
    "\n",
    "def reshape_to_dest_cube(cube):\n",
    "    return iris.util.new_axis(iris.util.new_axis(cube, 'forecast_period'), 'forecast_reference_time')\n",
    "\n",
    "\n",
    "def get_proto_zarr_array(meta):\n",
    "    OffSetS3Map(root=get_zar_path(meta), temp_chunk_path=meta['name'], check=False)\n",
    "    array_store = OffSetS3Map(root=get_zar_path(meta) +'/' + meta['name'], temp_chunk_path='', check=False)\n",
    "    return zarr.open(array_store)\n",
    "\n",
    "def meta_from_zarr_path(path):\n",
    "    decompose = path\n",
    "    decompose = decompose.rsplit('.',1)[0]\n",
    "    meta = {}\n",
    "    \n",
    "    models = ['mo-atmospheric-global-prd', 'mo-atmospheric-mogreps-g-prd', 'mo-atmospheric-ukv-prd', 'mo-atmospheric-mogreps-uk-prd']\n",
    "    for possiable_model in models:\n",
    "        if path.startswith(possiable_model):\n",
    "            model = possiable_model\n",
    "        \n",
    "    meta['model'] = model\n",
    "    \n",
    "    decompose = decompose.replace(model+'-', '')\n",
    "    \n",
    "    if path.find('-at_heights'):\n",
    "        height = \"0 1 2 3\"\n",
    "        meta['height'] = height\n",
    "        decompose = decompose.replace('-at_heights', '')\n",
    "        \n",
    "    if path.find('-at_pressures') > 0:\n",
    "        pressure = \"1000 2000 3000\" \n",
    "        meta['pressure'] = pressure\n",
    "        decompose = decompose.replace('-at_pressures', '')\n",
    "        \n",
    "    meta['name'] = decompose \n",
    "    \n",
    "    return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the zarr metadata\n",
    "The only axis that we need to update is the `forecast_reference_time`. Looking at the latest run below we can see that for the final forecast step the created time lags the forecast time by about 4-5 hours. Using that we can use the current time, derive what we expect the latest `forecast_reference_time` to be and then calculate what the dimention size needs to be (from the `origin`) to account for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Earliest run: 2019-02-14 03:00:00+00:00\n",
      "Latest run:   2019-02-15 03:00:00+00:00                           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def estimate_latest_avaliable_run(meta):\n",
    "    if not meta['model'] == \"mo-atmospheric-mogreps-uk-prd\":\n",
    "        raise RuntimeError(f\"Can only guess latest run for model mo-atmospheric-mogreps-uk-prd. Got {meta['model']}\")\n",
    "        \n",
    "        \n",
    "    fcst_ref_time = datetime.now().astimezone(timezone.utc) - timedelta(hours=5) # about a 5 hour delay from run time to being up.\n",
    "    while fcst_ref_time.hour not in  [3, 9, 15, 21] :\n",
    "        fcst_ref_time = fcst_ref_time - timedelta(hours=1)\n",
    "    fcst_ref_time = fcst_ref_time.replace(minute=0, second=0, microsecond=0)\n",
    "    return fcst_ref_time \n",
    "                           \n",
    "def estimate_earliest_avaliable_run(meta):\n",
    "    # Assume a 24 hour rolling window.\n",
    "    return estimate_latest_avaliable_run(meta) - timedelta(hours=24)\n",
    "\n",
    "print(f\"\"\"\n",
    "Earliest run: {estimate_earliest_avaliable_run({'model':\"mo-atmospheric-mogreps-uk-prd\"})}\n",
    "Latest run:   {estimate_latest_avaliable_run({'model':\"mo-atmospheric-mogreps-uk-prd\"})}                           \n",
    "\"\"\")        \n",
    "                 \n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'2019-02-11T15:00:00Z', \n",
    "'2019-02-11T21:00:00Z',\n",
    "'2019-02-12T03:00:00Z',\n",
    "'2019-02-12T09:00:00Z'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we know what data we think should exist it's a matter of working out the length along this axis that the zarr metadata needs to grow to include this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "zarr_path = 'mo-atmospheric-mogreps-uk-prd-air_temperature-at_heights.zarr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = meta_from_zarr_path(zarr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_zarr = get_proto_zarr_array(msg)\n",
    "origin = dest_zarr.attrs['_origin']\n",
    "offsets = []\n",
    "\n",
    "\n",
    "fcst_ref_t_dim = None\n",
    "for i, dim in enumerate(origin):\n",
    "    if dim['name'] == 'forecast_reference_time':\n",
    "        fcst_ref_t_dim_index, fcst_ref_t_dim = i, dim\n",
    "    if dim['name'] == 'forecast_period':\n",
    "        forecast_period_index, forecast_period_dim = i, dim\n",
    "\n",
    "\n",
    "assert fcst_ref_t_dim is not None, \"couldn't find dim fcst_ref_time in _origin attr of zarr\"\n",
    "assert forecast_period_dim is not None, \"couldn't find dim fcst_ref_time in _origin attr of zarr\"\n",
    " \n",
    "\n",
    "time_unit = cf_units.Unit(fcst_ref_t_dim['unit'])\n",
    "earlist_run = estimate_earliest_avaliable_run(msg)\n",
    "diff_from_origin =  time_unit.date2num(earlist_run)  - fcst_ref_t_dim['at']\n",
    "steps_from_origin = diff_from_origin / fcst_ref_t_dim['step']\n",
    "assert steps_from_origin % 1 == 0\n",
    "steps_from_origin = int(steps_from_origin)\n",
    "steps_from_origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the offset of the zarr along index `fcst_ref_t_dim_index` needs to be set as `steps_from_origin`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original offset: [110, 0, 0, 0, 0, 0]\n",
      "new offset: [120, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "offset = dest_zarr.attrs['_offset']\n",
    "print(f\"original offset: {offset}\")\n",
    "offset[fcst_ref_t_dim_index] = steps_from_origin\n",
    "print(f\"new offset: {offset}\")\n",
    "dest_zarr.attrs['_offset'] = offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zarr now grown but the metadata zarrs that xarray uses not in sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zarr.core.Array (4,) int64>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_reference_time_array = zarr.open(s3fs.S3Map(f's3://{get_zar_path(msg)}/forecast_reference_time'))\n",
    "forecast_reference_time_array \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2019, 2, 14, 3, 0, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2019, 2, 14, 9, 0, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2019, 2, 14, 15, 0, tzinfo=datetime.timezone.utc),\n",
       " datetime.datetime(2019, 2, 14, 21, 0, tzinfo=datetime.timezone.utc)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert msg['model'] == \"mo-atmospheric-mogreps-uk-prd\"\n",
    "MODEL_FREQUENCY_HOURS = 6\n",
    "ROLLING_WINDOW_HOURS = 24\n",
    "\n",
    "new_fcst_ref_times = [earlist_run + i * timedelta(hours=MODEL_FREQUENCY_HOURS) for i in range(ROLLING_WINDOW_HOURS//MODEL_FREQUENCY_HOURS )]\n",
    "new_fcst_ref_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unit('seconds since 1970-01-01', calendar='gregorian')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unit = cf_units.Unit(forecast_reference_time_array.attrs.get('units'))\n",
    "unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1550113200, 1550134800, 1550156400, 1550178000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert str(forecast_reference_time_array.dtype) in ['int32','int64']\n",
    "forecast_reference_time_array.resize((len(new_fcst_ref_times,)))\n",
    "forecast_reference_time_array[:] = [int(unit.date2num(time)) for time in new_fcst_ref_times]\n",
    "forecast_reference_time_array[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n"
     ]
    }
   ],
   "source": [
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
